{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Style Transfer Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The tutorial goes step-by-step through necessary steps to demonstrate object classification on images. Classification is performed running a pretrained network using the Intel® Distribution of OpenVINO™ toolkit OpenVINO™ Runtime.\n",
    "\n",
    "Style Transfer in Computer Vision is a task of composing images in the style of another image.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain Required Modules\n",
    "Install required modules on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python* modules that you will use in the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) is a standard Python module used to measure execution time.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [Deep Learning  OpenVINO™ Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [Matplotlib](https://matplotlib.org/) is a visualization module used to display output images.\n",
    "\n",
    "Run the cell below to import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _Optional_. Download and Convert a Pretrained Model from the Open Model Zoo\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [configuring inference](#3.-Configure-an-Inference).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [OpenVINO Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model, you need to configure a Python* environment to convert model from ONNX* framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m virtualenv /tmp/virtualenvs/tutorial_style_transfer\n",
    "source /tmp/virtualenvs/tutorial_style_transfer/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "pip uninstall openvino openvino_dev -y\n",
    "pip install --upgrade openvino-dev[onnx]==2022.1.0.dev20220316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `fast-neural-style-mosaic-onnx` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_style_transfer/bin/activate\n",
    "\n",
    "omz_downloader \\\n",
    "    --name fast-neural-style-mosaic-onnx \\\n",
    "    -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to translate the model into the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /tmp/virtualenvs/tutorial_style_transfer/bin/activate\n",
    "\n",
    "omz_converter \\\n",
    "    --name fast-neural-style-mosaic-onnx \\\n",
    "    -d raw_model \\\n",
    "    -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = \"model/public/fast-neural-style-mosaic-onnx/FP32/fast-neural-style-mosaic-onnx.xml\"\n",
    "model_bin = \"model/public/fast-neural-style-mosaic-onnx/FP32/fast-neural-style-mosaic-onnx.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "# Copy the path to one of images from the dataset imported in DL Workbench\n",
    "# or use the default image \"./tubingen.jpg\".\n",
    "input_image_path = \"tubingen.jpg\"\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tinput_image_path={input_image_path}\",\n",
    "    f\"\\n\\tdevice={device}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenVINO Runtime instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as `input_name`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "# Store the input name\n",
    "input_name = model.input().any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, c, h, w = model.input().get_shape()\n",
    "print(f\"Loaded the model into the OpenVINO Runtime for the {device} device.\", \n",
    "      f\"\\nModel input dimensions: n={n}, c={c}, h={h}, w={w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: Postprocessing is created only for the example `fast-neural-style-mosaic-onnx` style transfer model. If you used another model for this tutorial, rewrite the `process_and_display_results` function and, optionally, `load_labels_map` function. You can find style transfer model postprocessing examples in the OpenVINO samples</b>:\n",
    "\n",
    "[Style Transfer Python* Sample](https://docs.openvino.ai/latest/openvino_inference_engine_ie_bridges_python_sample_style_transfer_sample_README.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):\n",
    "    # Globals to store input width and height\n",
    "    global input_w, input_h\n",
    "    \n",
    "    # Use OpenCV to load the input image\n",
    "    img = cv2.imread(input_path)\n",
    "    \n",
    "    # Globals to store input width and height\n",
    "    input_h, input_w, *_ = img.shape\n",
    "\n",
    "    print(f\"Loaded the input image {input_path}. \\nInput image resolution: {input_w}x{input_h}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to resize the input image\n",
    "def resize_input_image(image):\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # Reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    print(f\"Resized input image to {w} x {h}\")\n",
    "    return in_frame\n",
    "\n",
    "# Load the image\n",
    "image = load_input_image(input_image_path)\n",
    "\n",
    "# Resize the input image\n",
    "in_frame = resize_input_image(image)\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infer the Model\n",
    "\n",
    "Now that you have the input image in the BGR format and of the right size, you can perform the inference of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request({input_name: in_frame})   \n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Show Predictions\n",
    "\n",
    "Output of a style transfer model is an image, therefore you need to apply only basic transformations to see the output next to the original image.\n",
    "\n",
    "Enjoy the way the model sees your image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to process inference results\n",
    "def process_results(res):\n",
    "    # Get the output\n",
    "    result = res[compiled_model.output()][0]\n",
    "    # Change the layout from CxHxW to HxWxC \n",
    "    result = np.transpose(result, (1, 2, 0))\n",
    "    \n",
    "    # Clip RGB values to the [0, 255] range\n",
    "    result = result.clip(min=0, max=255)\n",
    "\n",
    "    # Matplotlib expects a normalized image with pixel RGB values in the range [0,1].\n",
    "    result = result / 255\n",
    "    \n",
    "    # Resize the resulting image to match the original\n",
    "    result = cv2.resize(result, (input_w, input_h))\n",
    "    return result\n",
    "    \n",
    "# Create a function to process and display inference results\n",
    "def process_and_display_results(res, orig_input_image, orig_input_path, verbose=True):\n",
    "    # Display the original input image\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    im_to_show = cv2.cvtColor(orig_input_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(im_to_show)\n",
    "    \n",
    "    # Get the output\n",
    "    result = process_results(res)\n",
    "       \n",
    "    # Show the styled image\n",
    "    if verbose: print(f\"Results for the {orig_input_path} input image\")\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(result)\n",
    "\n",
    "process_and_display_results(res, image, input_image_path)\n",
    "print(\"Processed the image and displayed the inference output result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "384px",
    "width": "200px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

