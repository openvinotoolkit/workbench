{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Object Detection Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). This tutorial will go step-by-step through the necessary steps to demonstrate object detection on images. Object detection is performed using a pre-trained network and running it using the OpenVINO™ Runtime.\n",
    "\n",
    "Object Detection in Computer Vision is a task of finding objects and locating them in the image.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain Required Modules\n",
    "Install required modules on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python* modules that you will use in the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) is a standard Python module used to measure execution time.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [Deep Learning OpenVINO™ Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [Matplotlib](https://matplotlib.org/) is a visualization module used to display output images.\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from openvino.runtime import Core\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _Optional_. Download and Convert a Pretrained Model from the Open Model Zoo\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [configuring an inference](#3.-Configure-an-Inference).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the  [OpenVINO Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model, you need to configure a Python* environment to convert model from TensorFlow* framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m virtualenv /tmp/virtualenvs/tutorial_object_detection\n",
    "source /tmp/virtualenvs/tutorial_object_detection/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "pip uninstall openvino openvino_dev -y\n",
    "pip install --upgrade openvino-dev[tensorflow]==2022.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `ssd_mobilenet_v1_coco` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_object_detection/bin/activate\n",
    "\n",
    "omz_downloader --name ssd_mobilenet_v1_coco -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to translate the model into the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_object_detection/bin/activate\n",
    "\n",
    "omz_converter\\\n",
    "    --name ssd_mobilenet_v1_coco \\\n",
    "    -d raw_model \\\n",
    "    -o model \\\n",
    "    --precision FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = \"model/public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.xml\"\n",
    "model_bin = \"model/public/ssd_mobilenet_v1_coco/FP32/ssd_mobilenet_v1_coco.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**labels_path**| Path to the annotations file that maps the integers predicted by the model to strings. For example: `7: car`\n",
    "**prob_threshold**| Probability threshold to filter detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "# Copy the path to one of images from the dataset imported in DL Workbench\n",
    "# or use the default image \"./car.bmp\".\n",
    "input_image_path = \"car.bmp\"\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# Output labels file path or an empty string\n",
    "labels_path = \"labels.txt\"\n",
    "\n",
    "# Minimum probability threshold to detect an object\n",
    "prob_threshold = 0.5\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tinput_image_path={input_image_path}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    "    f\"\\n\\tlabels_path={labels_path}\", \n",
    "    f\"\\n\\tprob_threshold={prob_threshold}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenVINO™ Runtime instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as `input_name`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "# Store the input name\n",
    "input_name = model.input().any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, h, w, c = model.input().get_shape()\n",
    "print(f\"Loaded the model into the OpenVINO Runtime for the {device} device.\", \n",
    "      f\"\\nModel input dimensions: n={n}, c={c}, h={h}, w={w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):\n",
    "    # Globals to store input width and height\n",
    "    global input_w, input_h\n",
    "    \n",
    "    # Use OpenCV to load the input image\n",
    "    img = cv2.imread(input_path)\n",
    "    \n",
    "    input_h, input_w, *_ = img.shape\n",
    "    print(f\"Loaded the input image {input_path}. \\nInput image resolution: {input_w}x{input_h}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to resize the input image\n",
    "def resize_input_image(image):\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, h, w, c))\n",
    "    print(f\"Resized the input image to {w}x{h}.\")\n",
    "    return in_frame\n",
    "\n",
    "# Load the image\n",
    "image = load_input_image(input_image_path)\n",
    "\n",
    "# Resize the input image\n",
    "in_frame = resize_input_image(image)\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infer the Model\n",
    "\n",
    "Now that you have the input image in the BGR format and of the right size, you can perform the inference of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request({input_name: in_frame})   \n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Show Predictions\n",
    "\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\n",
    "\n",
    "A result of model inference (`res`) is an array of predictions. Each prediction `obj` has a following structure:\n",
    "\n",
    "- `obj[1]`: class ID, or the type of a detected object\n",
    "- `obj[2]`: Confidence level that currently detected object is an instance of the predicted class\n",
    "- `obj[3]`: lower x coordinate of the detected object \n",
    "- `obj[4]`: lower y coordinate of the detected object\n",
    "- `obj[5]`: upper x coordinate of the detected object\n",
    "- `obj[6]`: upper y coordinate of the detected object\n",
    "\n",
    "For each detected object, the output from the model will include an integer to indicate which type of the object, such as car or human, has been detected. To translate the integer into a more readable text string, use a label mapping file. The label mapping file is a text file of the format `n: string` (for example, `3: car`) that is loaded into a lookup table to be used later when labeling detected objects.\n",
    "\n",
    "Now we have an image where every detected object is bounded with a box with class id and confidence level. To replace class ids with their names, you need a label mapping file. You can find the sample label mapping file in the current directory with the name `labels.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: Postprocessing is created only for the example `ssd_mobilenet_v1_coco` object detection model. If you used another model for this tutorial, rewrite the `process_and_display_results` function and, optionally, `load_labels_map` function. You can find object detection model postprocessing examples in the OpenVINO samples</b>:\n",
    "\n",
    "- [Object Detection Python* Demo](https://docs.openvino.ai/latest/omz_demos_object_detection_demo_python.html)\n",
    "- [Object Detection SSD Python* Sample](https://docs.openvino.ai/latest/openvino_inference_engine_ie_bridges_python_sample_object_detection_sample_ssd_README.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_map():\n",
    "    labels_map = None\n",
    "    # If there is a path to a label mapping file, load the file into labels_map\n",
    "    print(labels_path)\n",
    "    if os.path.isfile(labels_path):\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n",
    "        print(f\"Loaded label mapping file [{labels_path}]\")\n",
    "    else:\n",
    "        print(\"No label mapping file has been loaded, only numbers will be used\",\n",
    "              \"for detected object labels.\")\n",
    "    return labels_map\n",
    "\n",
    "# Create a function to process inference results\n",
    "def process_results(result):\n",
    "    # Get output results\n",
    "    res = result[compiled_model.output()]\n",
    "    \n",
    "    # Load the names of the classes from the labels_path file if possible\n",
    "    labels_map = load_labels_map()\n",
    "    \n",
    "    # Loop through all possible results\n",
    "    for obj in res[0][0]:\n",
    "        # If probability is more than the specified threshold, draw and label the box \n",
    "        if obj[2] > prob_threshold:\n",
    "            # Get coordinates of the box containing the detected object\n",
    "            xmin = int(obj[3] * input_w)\n",
    "            ymin = int(obj[4] * input_h)\n",
    "            xmax = int(obj[5] * input_w)\n",
    "            ymax = int(obj[6] * input_h)\n",
    "            \n",
    "            # Get the type of the object detected\n",
    "            class_id = int(obj[1])\n",
    "            \n",
    "            # Draw the box and label for the detected object\n",
    "            color = (min(class_id * 12.5, 255), 255, 255)\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "            det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "            cv2.putText(image, det_label + ' ' + str(round(obj[2] * 100, 1)) + ' %', (xmin, ymin - 7),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "\n",
    "process_results(res)\n",
    "\n",
    "# Convert colors from BGR to RGB\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Disable axis display, then display the image\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "print(\"Processed the image and displayed the inference output result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "384px",
    "width": "200px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
