{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Learn OpenVINO™ C++ API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample computer vision application created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). \n",
    "\n",
    "The tutorial demonstrates how to build a simple computer vision application using OpenVINO C++ API. This tutorial was created for educational purposes and does not guarantee the immediate creation of a highly efficient application. You can build your own application based on the code from this tutorial. Find an example of a well-organized and architected solution that can be used as a basis for your future applications in the [Benchmark Tool documentation](https://docs.openvino.ai/latest/openvino_inference_engine_samples_benchmark_app_README.html).\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Learn about OpenVINO™ inference](#theory)\n",
    "2. [*Optional*. Download and convert a pretrained model from the Open Model Zoo](#model)\n",
    "3. [Build the executable](#build)\n",
    "4. [Execute the applicationn](#execute)\n",
    "5. [Explore the application](#experiment)\n",
    "6. [Experiment with Inference Engine C++ API ](#explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learn about OpenVINO™ Inference <a name=\"theory\"></a>\n",
    "\n",
    "Inference plays a crucial part in the OpenVINO toolkit. In this tutorial, we will cover only the basics using CPU specific terminology. Note that for devices other than the CPU, the terms may be different.\n",
    "\n",
    " You can find more detailed inference information in the documentation:\n",
    "- [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_samples_benchmark_app_README.html)\n",
    "- [Inference Engine Developer Guide](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n",
    "- [Overview of Inference Engine Plugin Library](https://docs.openvino.ai/latest/groupie_dev_api.html)\n",
    "\n",
    "The inference of a network is the execution of a computational graph consisting of different operations. Inference requests – abstraction over neural network execution (inference) used by OpenVINO [Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) (IE) runtime. Available cores are evenly distributed between the streams. Internally, the execution resources are split/pinned into execution streams.  \n",
    "\n",
    "Streams – number of inference requests running in parallel. \n",
    "Batch – number of images propagated to the network at a time. The time required to process one image is called Latency. The lower the value, the better.\n",
    "\n",
    "![](diagram_stream_batch.png)\n",
    "\n",
    "\n",
    "### Asynchronous API\n",
    "\n",
    "Asynchronous inference request runs an inference pipeline asynchronously in one or several task executors depending on a device pipeline structure. While some of the Infer Requests are processed by Inference Engine, the other ones can be filled with new frame data and asynchronously started, rather than wait for the inference to complete. Multiple requests are executed asynchronously and the throughput is measured in images per second by dividing the number of images that were processed by the processing time.\n",
    "\n",
    "For example, you can run inference and simultaneously encode the resulting or previous frames or run further inference, like emotion detection on top of the face detection results.\n",
    "\n",
    "**NOTE:** Changing the number of streams and batches usually is more effective in the high-performance applications, where a lot of images are being processed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. _Optional_. Download and Convert a Pretrained Model from the Open Model Zoo <a name=\"model\"></a>\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [the next step](#build).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html), which uses the IR model files to run inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model, you need to configure a Python* environment to convert the model from Caffe* framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install -r requirements.txt\n",
    "python3 -m virtualenv /tmp/virtualenvs/tutorial_sample_application\n",
    "source /tmp/virtualenvs/tutorial_sample_application/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "pip uninstall openvino openvino_dev -y\n",
    "pip install --upgrade openvino-dev[caffe]==2022.1.0.dev20220316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `squeezenet1.1` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_sample_application/bin/activate\n",
    "\n",
    "omz_downloader \\\n",
    "    --name squeezenet1.1 \\\n",
    "    -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to translate the model into the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /tmp/virtualenvs/tutorial_sample_application/bin/activate\n",
    "\n",
    "omz_converter \\\n",
    "    --name squeezenet1.1 \\\n",
    "    -d raw_model \\\n",
    "    -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Executable <a id=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# setup environment variables for build\n",
    "source $INTEL_OPENVINO_DIR/setupvars.sh\n",
    "\n",
    "# Remove previous assets if they exist\n",
    "rm -rf sample_app && rm -rf build\n",
    "\n",
    "# Create a directory for building\n",
    "mkdir build\n",
    "\n",
    "# Build the executable using 'cmake'\n",
    "cd build\n",
    "cmake ..\n",
    "cmake --build .\n",
    "\n",
    "# Copy the executable from the build directory\n",
    "cp sample_app .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute the Application <a id=\"execute\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the execution of a model, streams, as well as inference requests in a stream, can be distributed inefficiently among cores of hardware, which can reduce model speed. The optimal combination of batches and streams is specific to each particular accelerator. Therefore, the easiest way to speed up the model is to try different combinations. \n",
    "Refer to the documentation to [learn more](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html).\n",
    "\n",
    "Copy the configurations from DL Workbench:\n",
    "\n",
    "![](copy_sample.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Executable application accepts several arguments:\n",
    "# Path to the model (.xml)\n",
    "MODEL=\"model/public/squeezenet1.1/FP16/squeezenet1.1.xml\"\n",
    "# Batch size - how many images should be fed to the network at one time\n",
    "BATCH_SIZE=4\n",
    "# Number of streams\n",
    "STREAMS=5\n",
    "# Device\n",
    "DEVICE=\"CPU\"\n",
    "\n",
    "# Usage: ./sample_app path_to_model_xml number_of_batches number_of_streams\n",
    "./sample_app ${MODEL} ${BATCH_SIZE} ${STREAMS} ${DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore the Application <a id=\"explore\"></a> \n",
    "\n",
    "Inspect and change the [main](main.cpp) file with an application that uses the IE asynchronous C++ API.\n",
    "\n",
    "![](main_file.png)\n",
    "\n",
    "To check that you have learned the inference basics, open the file and try to answer the following questions:\n",
    "\n",
    "1. Where does the inference happen?\n",
    "2. Where the inference requests are created?\n",
    "3. Where does the processing of the results in the inference request happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment with Inference Engine C++ API <a id=\"experiment\"></a>\n",
    "\n",
    "To continue experimenting with the sample application, return to the [Step 3](#build) or proceed to explore DL Workbench functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning about OpenVINO™ inference, you can go to The [Benchmark Tool documentation](https://docs.openvino.ai/latest/openvino_inference_engine_samples_benchmark_app_README.html) to find an example of a well-structured solution that may be used as a basis for your future applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! Now you can proceed to building your own application on the basis of this tutorial and try numerous DL Workbench features, such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

