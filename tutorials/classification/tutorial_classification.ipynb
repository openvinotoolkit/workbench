{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The tutorial goes step-by-step through necessary steps to demonstrate object classification on images. Classification is performed running a pretrained network using the Intel® Distribution of OpenVINO™ toolkit Inference Engine.\n",
    "\n",
    "Classification in Computer Vision is a task of predicting the class of a specific object in an image.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain Required Modules\n",
    "Install required modules on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python* modules that you will use in the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) is a standard Python module used to measure execution time.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [Deep Learning Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [Matplotlib](https://matplotlib.org/) is a visualization module used to display output images.\n",
    "\n",
    "Run the cell below to import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _Optional_. Download and Convert a Pretrained Model from the Open Model Zoo\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [configuring an inference](#3.-Configure-an-Inference).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [Inference Engine](https://docs.openvino.ai/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model, you need to configure a Python* environment to convert model from Caffe* framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m virtualenv /tmp/virtualenvs/tutorial_classification\n",
    "source /tmp/virtualenvs/tutorial_classification/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "pip uninstall openvino openvino_dev -y\n",
    "pip install --upgrade openvino-dev[caffe]==2022.1.0.dev20220316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `squeezenet1.1` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_classification/bin/activate\n",
    "\n",
    "omz_downloader \\\n",
    "    --name squeezenet1.1 \\\n",
    "    -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to translate the model into the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /tmp/virtualenvs/tutorial_classification/bin/activate\n",
    "\n",
    "omz_converter \\\n",
    "    --name squeezenet1.1 \\\n",
    "    -d raw_model \\\n",
    "    -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = \"model/public/squeezenet1.1/FP32/squeezenet1.1.xml\"\n",
    "model_bin = \"model/public/squeezenet1.1/FP32/squeezenet1.1.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you define the path to the model, go to the [Initialize the OpenVINO™ Runtime](#Initialize-the-OpenVINO™-Runtime) step and execute all cells one by one to reach the [Show Predictions](#Show-Predictions) step. Then you can return and experiment with optional parameters below in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**report_top_n**| Number of top-N classification results to report.\n",
    "**labels_path**| Path to the label mapping file that maps model predictions to class names. Upload the label mapping file separately or provide the path your dataset imported in the DL Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "# Copy the path to one of images from the dataset imported in DL Workbench\n",
    "# or use the default image \"./car.bmp\".\n",
    "input_image_path = \"./car.bmp\"\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# Number of top results to display\n",
    "report_top_n = 10\n",
    "\n",
    "# Output labels file path or an empty string\n",
    "labels_path = 'labels.txt'\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tinput_image_path={input_image_path}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    "    f\"\\n\\tlabels_path={labels_path}\", \n",
    "    f\"\\n\\treport_top_n={report_top_n}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as `input_name`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "# Store the input name\n",
    "input_name = model.input().any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, c, h, w = model.input().get_shape()\n",
    "print(f\"Loaded the model into the Inference Engine for the {device} device.\", \n",
    "      f\"\\nModel input dimensions: n={n}, c={c}, h={h}, w={w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):\n",
    "    # Use OpenCV to load the input image\n",
    "    img = cv2.imread(input_path)\n",
    "    \n",
    "    input_h, input_w, *_ = img.shape\n",
    "    print(f\"Loaded the input image {input_path}. \\nInput image resolution: {input_w}x{input_h}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to resize the input image\n",
    "def resize_input_image(image):\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change the data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # Reshape to the input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    print(f\"Resized the input image to {w}x{h}.\")\n",
    "    return in_frame\n",
    "\n",
    "# Load image\n",
    "image = load_input_image(input_image_path)\n",
    "\n",
    "# Resize the input image\n",
    "in_frame = resize_input_image(image)\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infer the Model\n",
    "\n",
    "Now that you have the input image in the BGR format and of the right size, you can perform the inference of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request({input_name: in_frame})\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Show Predictions\n",
    "\n",
    "Now let's process the inference results by sorting all possible classes by the probabilities assigned during the inference, then selecting and displaying the top `report_top_n` items.\n",
    "\n",
    "For each detected object, the output from the model will include an integer to indicate which type of trained object has been detected, for example, daisy or bee. To translate the integer into a more readable text string, you can use a label mapping file. The label mapping file is a text file where a line number is used as an index of a lookup table to be used later when reporting detected objects.\n",
    "\n",
    "Here, if the `labels_path` variable has been set to point to a label mapping file, the file is opened and the labels are loaded into the `labels_map` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: Postprocessing is created only for the example `squeezenet1.1` classification model. If you used another model for this tutorial, rewrite the `process_and_display_results` function and, optionally, `load_labels_map` function. You can find classification model postprocessing examples in the OpenVINO samples</b>:\n",
    "- [Image Classification Async Python* Sample](https://docs.openvino.ai/latest/openvino_inference_engine_ie_bridges_python_sample_classification_sample_async_README.html)\n",
    "\n",
    "- [Hello Classification Python* Sample](https://docs.openvino.ai/latest/openvino_inference_engine_ie_bridges_python_sample_hello_classification_README.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_map():\n",
    "    labels_map = None\n",
    "    # If there is a path to a label mapping file, load the file into labels_map\n",
    "    if os.path.isfile(labels_path):\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n",
    "        print(f\"Loaded label mapping file [{labels_path}]\")\n",
    "    else:\n",
    "        print(\"No label mapping file has been loaded, only numbers will be used\",\n",
    "              \"for detected object labels.\")\n",
    "    return labels_map\n",
    "\n",
    "# Create a function to process inference results\n",
    "def process_and_display_results(probs, orig_input_image, orig_input_path):\n",
    "    # Load the classification labels from the labels_path file if possible\n",
    "    labels_map = load_labels_map()\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    im_to_show = cv2.cvtColor(orig_input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Report top N results for the image\n",
    "    print(f\"Top {report_top_n} results for the {orig_input_path} image:\")\n",
    "    \n",
    "    # Remove dimensions of length=1\n",
    "    probs = np.squeeze(probs)\n",
    "    \n",
    "    # Sort and return top report_top_n entries\n",
    "    top_ind = np.argsort(probs)[-report_top_n:][::-1]\n",
    "    \n",
    "    # Show the input image\n",
    "    plt.imshow(im_to_show)\n",
    "    \n",
    "    # Print out top probabilities, map labels\n",
    "    print(\"Probability % is <label>\")\n",
    "    for id in top_ind:\n",
    "        det_label = labels_map[id] if labels_map else f\"#{id}\"\n",
    "        print(f\" {probs[id] * 100:>10.2f} % is {det_label}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "process_and_display_results(res[compiled_model.output()], image, input_image_path)\n",
    "print(\"Processed the image and displayed the inference output result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

