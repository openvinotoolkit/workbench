{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Semantic Segmentation Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The tutorial goes step-by-step through necessary steps to demonstrate semantic segmentation on images. Semantic segmentation is performed running a pretrained network using the OpenVINO™ Runtime.\n",
    "\n",
    "Semantic segmentation in Computer Vision is a task of partitioning an image into multiple areas, each corresponding to a particular labeled object.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain Required Modules\n",
    "Install required modules on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python* modules that you will use in the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) is a standard Python module used to measure execution time.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [Deep Learning OpenVINO Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [Matplotlib](https://matplotlib.org/) is a visualization module used to display output images.\n",
    "\n",
    "Run the cell below to import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "from matplotlib import cm, pyplot as plt, patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _Optional_. Download and Convert a Pretrained Model from the Open Model Zoo\n",
    "\n",
    "> **NOTE**: If you already imported a model in the DL Workbench, skip this step and proceed to [configuring inference](#3.-Configure-an-Inference).\n",
    "\n",
    "OpenVINO™ toolkit includes the [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into Intermediate Representation (IR) model files, and the [OpenVINO Runtime](https://docs.openvino.ai/latest/openvino_docs_OV_Runtime_User_Guide.html), which uses the IR model files to run an inference on hardware devices. The IR model files are created from models trained in popular frameworks, like Caffe\\*, TensorFlow\\*, and others. \n",
    "\n",
    "OpenVINO™ [Model Downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html) downloads common inference models from the [Intel® Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before downloading a model you need to configure a Python* environment to convert model from TensorFlow* framework. To do this, create a new virtual environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m virtualenv /tmp/virtualenvs/tutorial_semantic_segmentation\n",
    "source /tmp/virtualenvs/tutorial_semantic_segmentation/bin/activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "pip uninstall openvino openvino_dev -y\n",
    "pip install --upgrade openvino-dev[tensorflow]==2022.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `deeplabv3` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source /tmp/virtualenvs/tutorial_semantic_segmentation/bin/activate\n",
    "\n",
    "omz_downloader \\\n",
    "    --name deeplabv3 \\\n",
    "    -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to translate the model into the OpenVINO™ IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /tmp/virtualenvs/tutorial_semantic_segmentation/bin/activate\n",
    "\n",
    "\n",
    "omz_converter \\\n",
    "    --name deeplabv3 \\\n",
    "    -d raw_model \\\n",
    "    -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = \"model/public/deeplabv3/FP32/deeplabv3.xml\"\n",
    "model_bin = \"model/public/deeplabv3/FP32/deeplabv3.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**labels_path**| Path to the annotations file that maps the integers predicted by the model to strings. For example: `7=car`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "# Copy the path to one of images from the dataset imported in DL Workbench\n",
    "# or use the default image \"./car.bmp\".\n",
    "input_image_path = \"car.bmp\"\n",
    "\n",
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# Output labels file path or an empty string\n",
    "labels_path = \"labels.txt\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tinput_image_path={input_image_path}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    "    f\"\\n\\tlabels_path={labels_path}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OpenVINO Runtime instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as `input_name`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "# Store the input name\n",
    "input_name = model.input().any_name\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, h, w, c = model.input().get_shape()\n",
    "print(f\"Loaded the model into the OpenVINO Runtime for the {device} device.\", \n",
    "      f\"\\nModel input dimensions: n={n}, c={c}, h={h}, w={w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):\n",
    "    # Use OpenCV to load the input image\n",
    "    img = cv2.imread(input_path)\n",
    "    \n",
    "    input_h, input_w, *_ = img.shape\n",
    "    print(f\"Loaded the input image {input_path}. \\nInput image resolution: {input_w}x{input_h}\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to resize the input image\n",
    "def resize_input_image(image):\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, h, w, c))\n",
    "    print(f\"Resized the input image to {w} x {h}\")\n",
    "    return in_frame\n",
    "\n",
    "# Load the image\n",
    "image = load_input_image(input_image_path)\n",
    "\n",
    "# Resize the input image\n",
    "in_frame = resize_input_image(image)\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Infer the Model\n",
    "\n",
    "Now that you have the input image in the BGR format and of the right size, you can perform the inference of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request({input_name: in_frame})\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Show Predictions\n",
    "\n",
    "The next step is to parse the inference results and draw masks over the corresponding areas of the image.\n",
    "\n",
    "A result of the model inference (`res`) is a segmentation mask that is a matrix of a size equal to the original image size. Each matrix element is an integer that indicates the class (for example, car, person, etc.) detected for the corresponding pixel of the original image.\n",
    "\n",
    "Now we have a segmented image with a legend that contains class ids. To replace class ids with their names, you need a label mapping file. You can find the sample label mapping file in the current directory with the name `labels.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note: Postprocessing is created only for the example `deeplabv3` semantic segmentation model. If you used another model for this tutorial, rewrite the `process_and_display_results` function and, optionally, `load_labels_map` function. You can find segmentation model postprocessing examples in the OpenVINO samples</b>:\n",
    "\n",
    "[Image Segmentation Python* Demo](https://docs.openvino.ai/latest/omz_demos_segmentation_demo_python.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_map():\n",
    "    labels_map = None\n",
    "    # If there is a path to a label mapping file, load the file into labels_map\n",
    "    print(labels_path)\n",
    "    if os.path.isfile(labels_path):\n",
    "        labels_map = {}\n",
    "        with open(labels_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        for line in content.split('\\n'):\n",
    "            if line:\n",
    "                key, value = line.split(sep='=', maxsplit=1)\n",
    "                labels_map[int(key)] = value\n",
    "        print(f\"Loaded label mapping file [{labels_path}]\")\n",
    "    else:\n",
    "        print(\"No label mapping file has been loaded, only numbers will be used\",\n",
    "              \"for detected object labels.\")\n",
    "    return labels_map\n",
    "\n",
    "# Create a function to process inference results\n",
    "def process_results(result):\n",
    "    # Get output results\n",
    "    res = result[compiled_model.output()]\n",
    "    \n",
    "    # Load the names of the classes from the labels_path file if possible\n",
    "    labels_map = load_labels_map()\n",
    "    \n",
    "    res = np.swapaxes(res, 0, 2)\n",
    "    res = np.swapaxes(res, 0, 1)\n",
    "    \n",
    "    resized = cv2.resize(image, res.shape[:2])\n",
    "    \n",
    "    class_ids = np.unique(res)\n",
    "    \n",
    "    #Disable axis display\n",
    "    plt.axis(\"off\")\n",
    "    # Display the original resized image\n",
    "    plt.imshow(resized)\n",
    "    # Display the network output as a half-transparent mask\n",
    "    im = plt.imshow(res, alpha=0.4)\n",
    "    \n",
    "    colors = [im.cmap(im.norm(value)) for value in class_ids]\n",
    "    patches = [\n",
    "        mpatches.Patch(\n",
    "            color=colors[i], \n",
    "            label=labels_map[cls] if labels_map else f'Class {cls}',\n",
    "        ) for i, cls in enumerate(class_ids)\n",
    "    ]\n",
    "    # Put the patches as legend-handles into the legend\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "process_results(res)\n",
    "print(\"Processed the image and displayed the inference output result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "384px",
    "width": "200px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
